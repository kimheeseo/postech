## 트랜스포머(transformer)
- https://wikidocs.net/31379
- https://nlpinkorean.github.io/illustrated-transformer/
- https://yngie-c.github.io/nlp/2020/07/01/nlp_transformer/
- https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding#4d058603-db0f-4d62-bb49-d85ea6dcbfc6

## Self-Attention과 Masked Self-Attention
- https://aimb.tistory.com/182

## seq2seq 모델
- https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

## word2vec
- https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/

## 멀티 헤드 어텐션(Multi-head Attention)
- https://velog.io/@cha-suyeon/%EB%A9%80%ED%8B%B0-%ED%97%A4%EB%93%9C-%EC%96%B4%ED%85%90%EC%85%98Multi-head-Attention-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0
- https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer3-Multi-Head-Attention%EA%B3%BC-Encoder

## 원핫인코딩
- https://wikidocs.net/22647

## positional Encoding
- [https://luminitworld.tistory.com/102](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)
