Path to model/logs: Results_ECCT\LDPC__Code_n_121_k_60__10_11_2022_07_27_01
Namespace(epochs=600, workers=0, lr=0.0001, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='LDPC', code_k=60, code_n=121, standardize=False, N_dec=4, d_model=32, h=8, code=<__main__.Code object at 0x000001D73A402470>, path='Results_ECCT\\LDPC__Code_n_121_k_60__10_11_2022_07_27_01')
Self-Attention Sparsity Ratio=74.55%, Self-Attention Complexity Ratio=12.72%
Mask:
 tensor([[[[False,  True,  True,  ...,  True,  True,  True],
          [ True, False,  True,  ...,  True,  True,  True],
          [ True,  True, False,  ...,  True,  True,  True],
          ...,
          [ True,  True,  True,  ..., False,  True,  True],
          [ True,  True,  True,  ...,  True, False,  True],
          [ True,  True,  True,  ...,  True,  True, False]]]])
ECC_Transformer(
  (decoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (oned_final_embed): Sequential(
    (0): Linear(in_features=32, out_features=1, bias=True)
  )
  (out_fc): Linear(in_features=187, out_features=121, bias=True)
)
# of Parameters: 79709
Training epoch 1, Batch 500/1000: LR=1.00e-04, Loss=2.51e-01 BER=8.09e-02 FER=9.61e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=2.23e-01 BER=6.68e-02 FER=9.58e-01
Epoch 1 Train Time 79.22583436965942s


Test Loss 4: 2.15e-01 5: 1.56e-01 6: 1.04e-01
Test FER 4: 9.99e-01 5: 9.91e-01 6: 9.43e-01
Test BER 4: 5.71e-02 5: 3.83e-02 6: 2.34e-02
Test -ln(BER) 4: 2.86e+00 5: 3.26e+00 6: 3.75e+00
# of testing samples: [100352.0, 100352.0, 100352.0]
 Test Time 100.76145720481873 s

Training epoch 2, Batch 500/1000: LR=1.00e-04, Loss=1.92e-01 BER=5.27e-02 FER=9.52e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=1.91e-01 BER=5.27e-02 FER=9.53e-01
Epoch 2 Train Time 76.1146788597107s

Training epoch 3, Batch 500/1000: LR=1.00e-04, Loss=1.83e-01 BER=5.28e-02 FER=9.53e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=1.78e-01 BER=5.23e-02 FER=9.32e-01
Epoch 3 Train Time 76.76426315307617s

Training epoch 4, Batch 500/1000: LR=1.00e-04, Loss=1.62e-01 BER=5.02e-02 FER=8.28e-01
Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=1.58e-01 BER=4.92e-02 FER=7.99e-01
Epoch 4 Train Time 76.22559952735901s

Training epoch 5, Batch 500/1000: LR=1.00e-04, Loss=1.46e-01 BER=4.71e-02 FER=7.37e-01
Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=1.41e-01 BER=4.62e-02 FER=7.19e-01
Epoch 5 Train Time 76.09927272796631s

Training epoch 6, Batch 500/1000: LR=1.00e-04, Loss=1.29e-01 BER=4.43e-02 FER=6.71e-01
Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=1.25e-01 BER=4.34e-02 FER=6.56e-01
Epoch 6 Train Time 76.8248610496521s

Training epoch 7, Batch 500/1000: LR=1.00e-04, Loss=1.15e-01 BER=4.09e-02 FER=6.07e-01
Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=1.12e-01 BER=4.02e-02 FER=5.95e-01
Epoch 7 Train Time 77.15059328079224s

Training epoch 8, Batch 500/1000: LR=1.00e-04, Loss=1.05e-01 BER=3.84e-02 FER=5.62e-01
Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=1.03e-01 BER=3.79e-02 FER=5.53e-01
Epoch 8 Train Time 77.18652105331421s

Training epoch 9, Batch 500/1000: LR=1.00e-04, Loss=9.86e-02 BER=3.66e-02 FER=5.28e-01
Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=9.73e-02 BER=3.62e-02 FER=5.23e-01
Epoch 9 Train Time 77.06783986091614s

Training epoch 10, Batch 500/1000: LR=9.99e-05, Loss=9.52e-02 BER=3.56e-02 FER=5.12e-01
Training epoch 10, Batch 1000/1000: LR=9.99e-05, Loss=9.41e-02 BER=3.52e-02 FER=5.08e-01
Epoch 10 Train Time 77.25334429740906s

Training epoch 11, Batch 500/1000: LR=9.99e-05, Loss=9.04e-02 BER=3.40e-02 FER=4.95e-01
Training epoch 11, Batch 1000/1000: LR=9.99e-05, Loss=8.97e-02 BER=3.37e-02 FER=4.93e-01
Epoch 11 Train Time 76.69384002685547s

Training epoch 12, Batch 500/1000: LR=9.99e-05, Loss=8.70e-02 BER=3.28e-02 FER=4.81e-01
Training epoch 12, Batch 1000/1000: LR=9.99e-05, Loss=8.60e-02 BER=3.25e-02 FER=4.77e-01
Epoch 12 Train Time 76.21212768554688s

Training epoch 13, Batch 500/1000: LR=9.99e-05, Loss=8.40e-02 BER=3.18e-02 FER=4.68e-01
Training epoch 13, Batch 1000/1000: LR=9.99e-05, Loss=8.27e-02 BER=3.14e-02 FER=4.62e-01
Epoch 13 Train Time 76.06449055671692s

Training epoch 14, Batch 500/1000: LR=9.99e-05, Loss=8.04e-02 BER=3.07e-02 FER=4.49e-01
Training epoch 14, Batch 1000/1000: LR=9.99e-05, Loss=7.93e-02 BER=3.04e-02 FER=4.43e-01
Epoch 14 Train Time 76.02887487411499s

Training epoch 15, Batch 500/1000: LR=9.99e-05, Loss=7.66e-02 BER=2.95e-02 FER=4.30e-01
Training epoch 15, Batch 1000/1000: LR=9.99e-05, Loss=7.60e-02 BER=2.93e-02 FER=4.26e-01
Epoch 15 Train Time 75.76179480552673s

Training epoch 16, Batch 500/1000: LR=9.98e-05, Loss=7.39e-02 BER=2.86e-02 FER=4.18e-01
Training epoch 16, Batch 1000/1000: LR=9.98e-05, Loss=7.36e-02 BER=2.85e-02 FER=4.17e-01
Epoch 16 Train Time 75.60037755966187s

Training epoch 17, Batch 500/1000: LR=9.98e-05, Loss=7.30e-02 BER=2.84e-02 FER=4.10e-01
Training epoch 17, Batch 1000/1000: LR=9.98e-05, Loss=7.24e-02 BER=2.82e-02 FER=4.08e-01
Epoch 17 Train Time 76.28787279129028s

Training epoch 18, Batch 500/1000: LR=9.98e-05, Loss=7.16e-02 BER=2.80e-02 FER=4.04e-01
Training epoch 18, Batch 1000/1000: LR=9.98e-05, Loss=7.12e-02 BER=2.79e-02 FER=4.02e-01
Epoch 18 Train Time 77.0014157295227s

Training epoch 19, Batch 500/1000: LR=9.98e-05, Loss=7.08e-02 BER=2.77e-02 FER=3.99e-01
Training epoch 19, Batch 1000/1000: LR=9.98e-05, Loss=7.05e-02 BER=2.76e-02 FER=3.98e-01
Epoch 19 Train Time 76.86397099494934s

Training epoch 20, Batch 500/1000: LR=9.98e-05, Loss=6.97e-02 BER=2.74e-02 FER=3.96e-01
Training epoch 20, Batch 1000/1000: LR=9.98e-05, Loss=6.95e-02 BER=2.73e-02 FER=3.94e-01
Epoch 20 Train Time 77.29506063461304s

Training epoch 21, Batch 500/1000: LR=9.97e-05, Loss=6.87e-02 BER=2.70e-02 FER=3.88e-01
Training epoch 21, Batch 1000/1000: LR=9.97e-05, Loss=6.85e-02 BER=2.69e-02 FER=3.87e-01
Epoch 21 Train Time 77.10016322135925s

Training epoch 22, Batch 500/1000: LR=9.97e-05, Loss=6.90e-02 BER=2.72e-02 FER=3.89e-01
Training epoch 22, Batch 1000/1000: LR=9.97e-05, Loss=6.84e-02 BER=2.70e-02 FER=3.86e-01
Epoch 22 Train Time 76.95990204811096s

Training epoch 23, Batch 500/1000: LR=9.97e-05, Loss=6.72e-02 BER=2.66e-02 FER=3.81e-01
Training epoch 23, Batch 1000/1000: LR=9.97e-05, Loss=6.78e-02 BER=2.68e-02 FER=3.82e-01
Epoch 23 Train Time 76.7136607170105s

Training epoch 24, Batch 500/1000: LR=9.96e-05, Loss=6.79e-02 BER=2.68e-02 FER=3.83e-01
Training epoch 24, Batch 1000/1000: LR=9.96e-05, Loss=6.77e-02 BER=2.67e-02 FER=3.82e-01
Epoch 24 Train Time 76.09466981887817s

Training epoch 25, Batch 500/1000: LR=9.96e-05, Loss=6.75e-02 BER=2.66e-02 FER=3.80e-01
Training epoch 25, Batch 1000/1000: LR=9.96e-05, Loss=6.70e-02 BER=2.64e-02 FER=3.78e-01
Epoch 25 Train Time 76.18532586097717s

Training epoch 26, Batch 500/1000: LR=9.96e-05, Loss=6.64e-02 BER=2.62e-02 FER=3.73e-01
Training epoch 26, Batch 1000/1000: LR=9.96e-05, Loss=6.64e-02 BER=2.62e-02 FER=3.75e-01
Epoch 26 Train Time 76.00044298171997s

Training epoch 27, Batch 500/1000: LR=9.95e-05, Loss=6.65e-02 BER=2.63e-02 FER=3.75e-01
Training epoch 27, Batch 1000/1000: LR=9.95e-05, Loss=6.71e-02 BER=2.65e-02 FER=3.76e-01
Epoch 27 Train Time 76.32527256011963s

Training epoch 28, Batch 500/1000: LR=9.95e-05, Loss=6.76e-02 BER=2.68e-02 FER=3.77e-01
Training epoch 28, Batch 1000/1000: LR=9.95e-05, Loss=6.68e-02 BER=2.64e-02 FER=3.74e-01
Epoch 28 Train Time 76.03249931335449s

Training epoch 29, Batch 500/1000: LR=9.95e-05, Loss=6.63e-02 BER=2.61e-02 FER=3.73e-01
Training epoch 29, Batch 1000/1000: LR=9.95e-05, Loss=6.62e-02 BER=2.61e-02 FER=3.73e-01
Epoch 29 Train Time 76.3376088142395s

Training epoch 30, Batch 500/1000: LR=9.94e-05, Loss=6.63e-02 BER=2.62e-02 FER=3.75e-01
Training epoch 30, Batch 1000/1000: LR=9.94e-05, Loss=6.62e-02 BER=2.62e-02 FER=3.73e-01
Epoch 30 Train Time 76.23190140724182s

Training epoch 31, Batch 500/1000: LR=9.94e-05, Loss=6.50e-02 BER=2.57e-02 FER=3.67e-01
Training epoch 31, Batch 1000/1000: LR=9.94e-05, Loss=6.57e-02 BER=2.60e-02 FER=3.70e-01
Epoch 31 Train Time 76.3519995212555s

Training epoch 32, Batch 500/1000: LR=9.93e-05, Loss=6.57e-02 BER=2.60e-02 FER=3.68e-01
Training epoch 32, Batch 1000/1000: LR=9.93e-05, Loss=6.57e-02 BER=2.60e-02 FER=3.69e-01
Epoch 32 Train Time 76.25462794303894s

Training epoch 33, Batch 500/1000: LR=9.93e-05, Loss=6.63e-02 BER=2.62e-02 FER=3.73e-01
Training epoch 33, Batch 1000/1000: LR=9.93e-05, Loss=6.58e-02 BER=2.60e-02 FER=3.70e-01
Epoch 33 Train Time 76.39187741279602s

Training epoch 34, Batch 500/1000: LR=9.93e-05, Loss=6.54e-02 BER=2.58e-02 FER=3.68e-01
Training epoch 34, Batch 1000/1000: LR=9.93e-05, Loss=6.57e-02 BER=2.60e-02 FER=3.69e-01
Epoch 34 Train Time 76.19804811477661s

Training epoch 35, Batch 500/1000: LR=9.92e-05, Loss=6.52e-02 BER=2.59e-02 FER=3.67e-01
Training epoch 35, Batch 1000/1000: LR=9.92e-05, Loss=6.54e-02 BER=2.59e-02 FER=3.68e-01
Epoch 35 Train Time 76.25964903831482s

Training epoch 36, Batch 500/1000: LR=9.92e-05, Loss=6.48e-02 BER=2.57e-02 FER=3.64e-01
Training epoch 36, Batch 1000/1000: LR=9.92e-05, Loss=6.53e-02 BER=2.59e-02 FER=3.66e-01
Epoch 36 Train Time 76.22915172576904s

Training epoch 37, Batch 500/1000: LR=9.91e-05, Loss=6.55e-02 BER=2.59e-02 FER=3.66e-01
Training epoch 37, Batch 1000/1000: LR=9.91e-05, Loss=6.53e-02 BER=2.58e-02 FER=3.66e-01
Epoch 37 Train Time 76.23361945152283s

Training epoch 38, Batch 500/1000: LR=9.91e-05, Loss=6.52e-02 BER=2.59e-02 FER=3.65e-01
Training epoch 38, Batch 1000/1000: LR=9.91e-05, Loss=6.53e-02 BER=2.58e-02 FER=3.65e-01
Epoch 38 Train Time 76.15273785591125s

Training epoch 39, Batch 500/1000: LR=9.90e-05, Loss=6.59e-02 BER=2.62e-02 FER=3.67e-01
Training epoch 39, Batch 1000/1000: LR=9.90e-05, Loss=6.54e-02 BER=2.59e-02 FER=3.66e-01
Epoch 39 Train Time 76.18550848960876s

Training epoch 40, Batch 500/1000: LR=9.90e-05, Loss=6.55e-02 BER=2.60e-02 FER=3.65e-01
Training epoch 40, Batch 1000/1000: LR=9.90e-05, Loss=6.50e-02 BER=2.58e-02 FER=3.64e-01
Epoch 40 Train Time 76.27532196044922s

Training epoch 41, Batch 500/1000: LR=9.89e-05, Loss=6.55e-02 BER=2.60e-02 FER=3.65e-01
Training epoch 41, Batch 1000/1000: LR=9.89e-05, Loss=6.55e-02 BER=2.60e-02 FER=3.65e-01
Epoch 41 Train Time 76.20991015434265s

Training epoch 42, Batch 500/1000: LR=9.89e-05, Loss=6.45e-02 BER=2.56e-02 FER=3.60e-01
Training epoch 42, Batch 1000/1000: LR=9.89e-05, Loss=6.45e-02 BER=2.56e-02 FER=3.60e-01
Epoch 42 Train Time 75.82102370262146s

Training epoch 43, Batch 500/1000: LR=9.88e-05, Loss=6.59e-02 BER=2.61e-02 FER=3.67e-01
Training epoch 43, Batch 1000/1000: LR=9.88e-05, Loss=6.56e-02 BER=2.60e-02 FER=3.65e-01
Epoch 43 Train Time 75.1649386882782s

Training epoch 44, Batch 500/1000: LR=9.88e-05, Loss=6.49e-02 BER=2.57e-02 FER=3.62e-01
Training epoch 44, Batch 1000/1000: LR=9.88e-05, Loss=6.49e-02 BER=2.57e-02 FER=3.62e-01
Epoch 44 Train Time 75.09963536262512s

Training epoch 45, Batch 500/1000: LR=9.87e-05, Loss=6.44e-02 BER=2.55e-02 FER=3.60e-01
Training epoch 45, Batch 1000/1000: LR=9.87e-05, Loss=6.47e-02 BER=2.57e-02 FER=3.61e-01
Epoch 45 Train Time 75.06716132164001s

Training epoch 46, Batch 500/1000: LR=9.86e-05, Loss=6.43e-02 BER=2.55e-02 FER=3.58e-01
Training epoch 46, Batch 1000/1000: LR=9.86e-05, Loss=6.44e-02 BER=2.55e-02 FER=3.59e-01
Epoch 46 Train Time 75.05224418640137s

Training epoch 47, Batch 500/1000: LR=9.86e-05, Loss=6.52e-02 BER=2.58e-02 FER=3.64e-01
Training epoch 47, Batch 1000/1000: LR=9.86e-05, Loss=6.51e-02 BER=2.58e-02 FER=3.63e-01
Epoch 47 Train Time 75.10873985290527s

Training epoch 48, Batch 500/1000: LR=9.85e-05, Loss=6.45e-02 BER=2.56e-02 FER=3.59e-01
Training epoch 48, Batch 1000/1000: LR=9.85e-05, Loss=6.42e-02 BER=2.54e-02 FER=3.58e-01
Epoch 48 Train Time 75.06403303146362s

Training epoch 49, Batch 500/1000: LR=9.84e-05, Loss=6.46e-02 BER=2.56e-02 FER=3.59e-01
Training epoch 49, Batch 1000/1000: LR=9.84e-05, Loss=6.45e-02 BER=2.55e-02 FER=3.59e-01
Epoch 49 Train Time 75.23623061180115s

Training epoch 50, Batch 500/1000: LR=9.84e-05, Loss=6.47e-02 BER=2.57e-02 FER=3.60e-01
Training epoch 50, Batch 1000/1000: LR=9.84e-05, Loss=6.45e-02 BER=2.56e-02 FER=3.58e-01
Epoch 50 Train Time 75.03666281700134s

Training epoch 51, Batch 500/1000: LR=9.83e-05, Loss=6.47e-02 BER=2.56e-02 FER=3.60e-01
Training epoch 51, Batch 1000/1000: LR=9.83e-05, Loss=6.45e-02 BER=2.55e-02 FER=3.59e-01
Epoch 51 Train Time 75.04868626594543s

Training epoch 52, Batch 500/1000: LR=9.82e-05, Loss=6.45e-02 BER=2.55e-02 FER=3.60e-01
Training epoch 52, Batch 1000/1000: LR=9.82e-05, Loss=6.46e-02 BER=2.56e-02 FER=3.60e-01
Epoch 52 Train Time 75.05670928955078s

Training epoch 53, Batch 500/1000: LR=9.82e-05, Loss=6.39e-02 BER=2.53e-02 FER=3.55e-01
Training epoch 53, Batch 1000/1000: LR=9.82e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.56e-01
Epoch 53 Train Time 75.09800481796265s

Training epoch 54, Batch 500/1000: LR=9.81e-05, Loss=6.49e-02 BER=2.57e-02 FER=3.62e-01
Training epoch 54, Batch 1000/1000: LR=9.81e-05, Loss=6.44e-02 BER=2.55e-02 FER=3.59e-01
Epoch 54 Train Time 75.20795106887817s

Training epoch 55, Batch 500/1000: LR=9.80e-05, Loss=6.46e-02 BER=2.56e-02 FER=3.59e-01
Training epoch 55, Batch 1000/1000: LR=9.80e-05, Loss=6.44e-02 BER=2.55e-02 FER=3.58e-01
Epoch 55 Train Time 75.03297638893127s

Training epoch 56, Batch 500/1000: LR=9.80e-05, Loss=6.43e-02 BER=2.55e-02 FER=3.58e-01
Training epoch 56, Batch 1000/1000: LR=9.80e-05, Loss=6.42e-02 BER=2.55e-02 FER=3.58e-01
Epoch 56 Train Time 75.02128601074219s

Training epoch 57, Batch 500/1000: LR=9.79e-05, Loss=6.41e-02 BER=2.53e-02 FER=3.57e-01
Training epoch 57, Batch 1000/1000: LR=9.79e-05, Loss=6.45e-02 BER=2.55e-02 FER=3.58e-01
Epoch 57 Train Time 75.03108286857605s

Training epoch 58, Batch 500/1000: LR=9.78e-05, Loss=6.39e-02 BER=2.53e-02 FER=3.56e-01
Training epoch 58, Batch 1000/1000: LR=9.78e-05, Loss=6.40e-02 BER=2.53e-02 FER=3.57e-01
Epoch 58 Train Time 75.03873109817505s

Training epoch 59, Batch 500/1000: LR=9.77e-05, Loss=6.50e-02 BER=2.58e-02 FER=3.59e-01
Training epoch 59, Batch 1000/1000: LR=9.77e-05, Loss=6.44e-02 BER=2.56e-02 FER=3.59e-01
Epoch 59 Train Time 75.1922357082367s

Training epoch 60, Batch 500/1000: LR=9.77e-05, Loss=6.39e-02 BER=2.53e-02 FER=3.55e-01
Training epoch 60, Batch 1000/1000: LR=9.77e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.55e-01
Epoch 60 Train Time 75.03519916534424s

Training epoch 61, Batch 500/1000: LR=9.76e-05, Loss=6.44e-02 BER=2.55e-02 FER=3.60e-01
Training epoch 61, Batch 1000/1000: LR=9.76e-05, Loss=6.41e-02 BER=2.54e-02 FER=3.58e-01
Epoch 61 Train Time 75.16294980049133s

Training epoch 62, Batch 500/1000: LR=9.75e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.57e-01
Training epoch 62, Batch 1000/1000: LR=9.75e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.57e-01
Epoch 62 Train Time 75.03236198425293s

Training epoch 63, Batch 500/1000: LR=9.74e-05, Loss=6.36e-02 BER=2.52e-02 FER=3.54e-01
Training epoch 63, Batch 1000/1000: LR=9.74e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.56e-01
Epoch 63 Train Time 75.17790174484253s

Training epoch 64, Batch 500/1000: LR=9.73e-05, Loss=6.42e-02 BER=2.55e-02 FER=3.56e-01
Training epoch 64, Batch 1000/1000: LR=9.73e-05, Loss=6.42e-02 BER=2.55e-02 FER=3.56e-01
Epoch 64 Train Time 76.44025921821594s

Training epoch 65, Batch 500/1000: LR=9.72e-05, Loss=6.43e-02 BER=2.56e-02 FER=3.55e-01
Training epoch 65, Batch 1000/1000: LR=9.72e-05, Loss=6.43e-02 BER=2.55e-02 FER=3.56e-01
Epoch 65 Train Time 76.39537453651428s

Training epoch 66, Batch 500/1000: LR=9.72e-05, Loss=6.45e-02 BER=2.56e-02 FER=3.57e-01
Training epoch 66, Batch 1000/1000: LR=9.72e-05, Loss=6.43e-02 BER=2.55e-02 FER=3.56e-01
Epoch 66 Train Time 76.27168011665344s

Training epoch 67, Batch 500/1000: LR=9.71e-05, Loss=6.40e-02 BER=2.55e-02 FER=3.55e-01
Training epoch 67, Batch 1000/1000: LR=9.71e-05, Loss=6.41e-02 BER=2.54e-02 FER=3.56e-01
Epoch 67 Train Time 76.78338670730591s

Training epoch 68, Batch 500/1000: LR=9.70e-05, Loss=6.46e-02 BER=2.57e-02 FER=3.57e-01
Training epoch 68, Batch 1000/1000: LR=9.70e-05, Loss=6.42e-02 BER=2.54e-02 FER=3.55e-01
Epoch 68 Train Time 76.94001078605652s

Training epoch 69, Batch 500/1000: LR=9.69e-05, Loss=6.43e-02 BER=2.55e-02 FER=3.56e-01
Training epoch 69, Batch 1000/1000: LR=9.69e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.54e-01
Epoch 69 Train Time 77.15772104263306s

Training epoch 70, Batch 500/1000: LR=9.68e-05, Loss=6.41e-02 BER=2.54e-02 FER=3.55e-01
Training epoch 70, Batch 1000/1000: LR=9.68e-05, Loss=6.39e-02 BER=2.53e-02 FER=3.54e-01
Epoch 70 Train Time 76.99990129470825s

Training epoch 71, Batch 500/1000: LR=9.67e-05, Loss=6.38e-02 BER=2.53e-02 FER=3.54e-01
Training epoch 71, Batch 1000/1000: LR=9.67e-05, Loss=6.36e-02 BER=2.52e-02 FER=3.52e-01
Epoch 71 Train Time 76.81890630722046s

Training epoch 72, Batch 500/1000: LR=9.66e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.55e-01
Training epoch 72, Batch 1000/1000: LR=9.66e-05, Loss=6.38e-02 BER=2.53e-02 FER=3.54e-01
Epoch 72 Train Time 76.01807999610901s

Training epoch 73, Batch 500/1000: LR=9.65e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.56e-01
Training epoch 73, Batch 1000/1000: LR=9.65e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.54e-01
Epoch 73 Train Time 75.98658061027527s

Training epoch 74, Batch 500/1000: LR=9.64e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.54e-01
Training epoch 74, Batch 1000/1000: LR=9.64e-05, Loss=6.41e-02 BER=2.54e-02 FER=3.55e-01
Epoch 74 Train Time 76.42711687088013s

Training epoch 75, Batch 500/1000: LR=9.63e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.52e-01
Training epoch 75, Batch 1000/1000: LR=9.63e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.53e-01
Epoch 75 Train Time 75.43425512313843s

Training epoch 76, Batch 500/1000: LR=9.62e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.54e-01
Training epoch 76, Batch 1000/1000: LR=9.62e-05, Loss=6.36e-02 BER=2.53e-02 FER=3.53e-01
Epoch 76 Train Time 75.46496510505676s

Training epoch 77, Batch 500/1000: LR=9.61e-05, Loss=6.39e-02 BER=2.53e-02 FER=3.55e-01
Training epoch 77, Batch 1000/1000: LR=9.61e-05, Loss=6.42e-02 BER=2.54e-02 FER=3.55e-01
Epoch 77 Train Time 75.41548418998718s

Training epoch 78, Batch 500/1000: LR=9.60e-05, Loss=6.41e-02 BER=2.54e-02 FER=3.53e-01
Training epoch 78, Batch 1000/1000: LR=9.60e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.53e-01
Epoch 78 Train Time 75.37501907348633s

Training epoch 79, Batch 500/1000: LR=9.59e-05, Loss=6.39e-02 BER=2.53e-02 FER=3.53e-01
Training epoch 79, Batch 1000/1000: LR=9.59e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.54e-01
Epoch 79 Train Time 75.22647190093994s

Training epoch 80, Batch 500/1000: LR=9.58e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.51e-01
Training epoch 80, Batch 1000/1000: LR=9.58e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.52e-01
Epoch 80 Train Time 75.18925714492798s

Training epoch 81, Batch 500/1000: LR=9.57e-05, Loss=6.43e-02 BER=2.55e-02 FER=3.56e-01
Training epoch 81, Batch 1000/1000: LR=9.57e-05, Loss=6.38e-02 BER=2.53e-02 FER=3.53e-01
Epoch 81 Train Time 75.31417512893677s

Training epoch 82, Batch 500/1000: LR=9.56e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.51e-01
Training epoch 82, Batch 1000/1000: LR=9.56e-05, Loss=6.35e-02 BER=2.52e-02 FER=3.51e-01
Epoch 82 Train Time 75.17781209945679s

Training epoch 83, Batch 500/1000: LR=9.55e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.50e-01
Training epoch 83, Batch 1000/1000: LR=9.55e-05, Loss=6.31e-02 BER=2.50e-02 FER=3.50e-01
Epoch 83 Train Time 75.19123482704163s

Training epoch 84, Batch 500/1000: LR=9.54e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.49e-01
Training epoch 84, Batch 1000/1000: LR=9.54e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.49e-01
Epoch 84 Train Time 75.33076167106628s

Training epoch 85, Batch 500/1000: LR=9.53e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.50e-01
Training epoch 85, Batch 1000/1000: LR=9.53e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.50e-01
Epoch 85 Train Time 75.78477096557617s

Training epoch 86, Batch 500/1000: LR=9.52e-05, Loss=6.36e-02 BER=2.53e-02 FER=3.51e-01
Training epoch 86, Batch 1000/1000: LR=9.52e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.52e-01
Epoch 86 Train Time 75.57615280151367s

Training epoch 87, Batch 500/1000: LR=9.51e-05, Loss=6.38e-02 BER=2.53e-02 FER=3.50e-01
Training epoch 87, Batch 1000/1000: LR=9.51e-05, Loss=6.38e-02 BER=2.53e-02 FER=3.51e-01
Epoch 87 Train Time 75.56793475151062s

Training epoch 88, Batch 500/1000: LR=9.50e-05, Loss=6.32e-02 BER=2.52e-02 FER=3.50e-01
Training epoch 88, Batch 1000/1000: LR=9.50e-05, Loss=6.33e-02 BER=2.51e-02 FER=3.49e-01
Epoch 88 Train Time 75.55987429618835s

Training epoch 89, Batch 500/1000: LR=9.48e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.52e-01
Training epoch 89, Batch 1000/1000: LR=9.48e-05, Loss=6.39e-02 BER=2.54e-02 FER=3.53e-01
Epoch 89 Train Time 75.57590436935425s

Training epoch 90, Batch 500/1000: LR=9.47e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.51e-01
Training epoch 90, Batch 1000/1000: LR=9.47e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.51e-01
Epoch 90 Train Time 75.61590623855591s

Training epoch 91, Batch 500/1000: LR=9.46e-05, Loss=6.23e-02 BER=2.48e-02 FER=3.45e-01
Training epoch 91, Batch 1000/1000: LR=9.46e-05, Loss=6.23e-02 BER=2.48e-02 FER=3.45e-01
Epoch 91 Train Time 75.55990266799927s

Training epoch 92, Batch 500/1000: LR=9.45e-05, Loss=6.38e-02 BER=2.54e-02 FER=3.51e-01
Training epoch 92, Batch 1000/1000: LR=9.45e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.50e-01
Epoch 92 Train Time 75.64787530899048s

Training epoch 93, Batch 500/1000: LR=9.44e-05, Loss=6.38e-02 BER=2.54e-02 FER=3.51e-01
Training epoch 93, Batch 1000/1000: LR=9.44e-05, Loss=6.35e-02 BER=2.53e-02 FER=3.51e-01
Epoch 93 Train Time 75.73590397834778s

Training epoch 94, Batch 500/1000: LR=9.42e-05, Loss=6.40e-02 BER=2.54e-02 FER=3.52e-01
Training epoch 94, Batch 1000/1000: LR=9.42e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.50e-01
Epoch 94 Train Time 75.47992777824402s

Training epoch 95, Batch 500/1000: LR=9.41e-05, Loss=6.28e-02 BER=2.49e-02 FER=3.46e-01
Training epoch 95, Batch 1000/1000: LR=9.41e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.47e-01
Epoch 95 Train Time 75.41588139533997s

Training epoch 96, Batch 500/1000: LR=9.40e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.46e-01
Training epoch 96, Batch 1000/1000: LR=9.40e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.48e-01
Epoch 96 Train Time 75.39990520477295s

Training epoch 97, Batch 500/1000: LR=9.39e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.50e-01
Training epoch 97, Batch 1000/1000: LR=9.39e-05, Loss=6.33e-02 BER=2.51e-02 FER=3.49e-01
Epoch 97 Train Time 75.40794658660889s

Training epoch 98, Batch 500/1000: LR=9.38e-05, Loss=6.35e-02 BER=2.52e-02 FER=3.50e-01
Training epoch 98, Batch 1000/1000: LR=9.38e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.48e-01
Epoch 98 Train Time 75.39989709854126s

Training epoch 99, Batch 500/1000: LR=9.36e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.49e-01
Training epoch 99, Batch 1000/1000: LR=9.36e-05, Loss=6.33e-02 BER=2.51e-02 FER=3.48e-01
Epoch 99 Train Time 75.43987011909485s

Training epoch 100, Batch 500/1000: LR=9.35e-05, Loss=6.35e-02 BER=2.52e-02 FER=3.48e-01
Training epoch 100, Batch 1000/1000: LR=9.35e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.49e-01
Epoch 100 Train Time 75.3839054107666s

Training epoch 101, Batch 500/1000: LR=9.34e-05, Loss=6.30e-02 BER=2.51e-02 FER=3.47e-01
Training epoch 101, Batch 1000/1000: LR=9.34e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.49e-01
Epoch 101 Train Time 75.45590353012085s

Training epoch 102, Batch 500/1000: LR=9.32e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.49e-01
Training epoch 102, Batch 1000/1000: LR=9.32e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.50e-01
Epoch 102 Train Time 75.39990496635437s

Training epoch 103, Batch 500/1000: LR=9.31e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.48e-01
Training epoch 103, Batch 1000/1000: LR=9.31e-05, Loss=6.33e-02 BER=2.51e-02 FER=3.48e-01
Epoch 103 Train Time 75.43990468978882s

Training epoch 104, Batch 500/1000: LR=9.30e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.46e-01
Training epoch 104, Batch 1000/1000: LR=9.30e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.47e-01
Epoch 104 Train Time 75.45590281486511s

Training epoch 105, Batch 500/1000: LR=9.28e-05, Loss=6.24e-02 BER=2.48e-02 FER=3.45e-01
Training epoch 105, Batch 1000/1000: LR=9.28e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.45e-01
Epoch 105 Train Time 75.41593432426453s

Training epoch 106, Batch 500/1000: LR=9.27e-05, Loss=6.30e-02 BER=2.51e-02 FER=3.47e-01
Training epoch 106, Batch 1000/1000: LR=9.27e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.46e-01
Epoch 106 Train Time 75.38393211364746s

Training epoch 107, Batch 500/1000: LR=9.26e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.48e-01
Training epoch 107, Batch 1000/1000: LR=9.26e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.48e-01
Epoch 107 Train Time 75.43987464904785s

Training epoch 108, Batch 500/1000: LR=9.24e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.47e-01
Training epoch 108, Batch 1000/1000: LR=9.24e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.48e-01
Epoch 108 Train Time 75.43190455436707s

Training epoch 109, Batch 500/1000: LR=9.23e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.47e-01
Training epoch 109, Batch 1000/1000: LR=9.23e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.46e-01
Epoch 109 Train Time 75.4399061203003s

Training epoch 110, Batch 500/1000: LR=9.22e-05, Loss=6.33e-02 BER=2.53e-02 FER=3.47e-01
Training epoch 110, Batch 1000/1000: LR=9.22e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.46e-01
Epoch 110 Train Time 75.39993333816528s

Training epoch 111, Batch 500/1000: LR=9.20e-05, Loss=6.37e-02 BER=2.53e-02 FER=3.49e-01
Training epoch 111, Batch 1000/1000: LR=9.20e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.47e-01
Epoch 111 Train Time 75.40790295600891s

Training epoch 112, Batch 500/1000: LR=9.19e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.46e-01
Training epoch 112, Batch 1000/1000: LR=9.19e-05, Loss=6.36e-02 BER=2.53e-02 FER=3.48e-01
Epoch 112 Train Time 75.43987822532654s

Training epoch 113, Batch 500/1000: LR=9.17e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.47e-01
Training epoch 113, Batch 1000/1000: LR=9.17e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.47e-01
Epoch 113 Train Time 75.47190284729004s

Training epoch 114, Batch 500/1000: LR=9.16e-05, Loss=6.33e-02 BER=2.51e-02 FER=3.46e-01
Training epoch 114, Batch 1000/1000: LR=9.16e-05, Loss=6.34e-02 BER=2.52e-02 FER=3.47e-01
Epoch 114 Train Time 75.43193030357361s

Training epoch 115, Batch 500/1000: LR=9.14e-05, Loss=6.35e-02 BER=2.52e-02 FER=3.48e-01
Training epoch 115, Batch 1000/1000: LR=9.14e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.48e-01
Epoch 115 Train Time 75.40790224075317s

Training epoch 116, Batch 500/1000: LR=9.13e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.48e-01
Training epoch 116, Batch 1000/1000: LR=9.13e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.47e-01
Epoch 116 Train Time 75.40788125991821s

Training epoch 117, Batch 500/1000: LR=9.11e-05, Loss=6.24e-02 BER=2.48e-02 FER=3.44e-01
Training epoch 117, Batch 1000/1000: LR=9.11e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.45e-01
Epoch 117 Train Time 75.39990448951721s

Training epoch 118, Batch 500/1000: LR=9.10e-05, Loss=6.25e-02 BER=2.48e-02 FER=3.46e-01
Training epoch 118, Batch 1000/1000: LR=9.10e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.46e-01
Epoch 118 Train Time 75.47990441322327s

Training epoch 119, Batch 500/1000: LR=9.08e-05, Loss=6.23e-02 BER=2.47e-02 FER=3.43e-01
Training epoch 119, Batch 1000/1000: LR=9.08e-05, Loss=6.26e-02 BER=2.48e-02 FER=3.45e-01
Epoch 119 Train Time 75.38393950462341s

Training epoch 120, Batch 500/1000: LR=9.07e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.48e-01
Training epoch 120, Batch 1000/1000: LR=9.07e-05, Loss=6.27e-02 BER=2.49e-02 FER=3.46e-01
Epoch 120 Train Time 75.41589736938477s

Training epoch 121, Batch 500/1000: LR=9.05e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.47e-01
Training epoch 121, Batch 1000/1000: LR=9.05e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.46e-01
Epoch 121 Train Time 75.4079327583313s

Training epoch 122, Batch 500/1000: LR=9.04e-05, Loss=6.28e-02 BER=2.49e-02 FER=3.46e-01
Training epoch 122, Batch 1000/1000: LR=9.04e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.45e-01
Epoch 122 Train Time 75.39189171791077s

Training epoch 123, Batch 500/1000: LR=9.02e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.47e-01
Training epoch 123, Batch 1000/1000: LR=9.02e-05, Loss=6.31e-02 BER=2.52e-02 FER=3.47e-01
Epoch 123 Train Time 75.40017223358154s

Training epoch 124, Batch 500/1000: LR=9.01e-05, Loss=6.20e-02 BER=2.47e-02 FER=3.43e-01
Training epoch 124, Batch 1000/1000: LR=9.01e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.46e-01
Epoch 124 Train Time 75.44790554046631s

Training epoch 125, Batch 500/1000: LR=8.99e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.46e-01
Training epoch 125, Batch 1000/1000: LR=8.99e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.45e-01
Epoch 125 Train Time 75.43990302085876s

Training epoch 126, Batch 500/1000: LR=8.98e-05, Loss=6.25e-02 BER=2.48e-02 FER=3.44e-01
Training epoch 126, Batch 1000/1000: LR=8.98e-05, Loss=6.24e-02 BER=2.48e-02 FER=3.44e-01
Epoch 126 Train Time 75.39190483093262s

Training epoch 127, Batch 500/1000: LR=8.96e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.45e-01
Training epoch 127, Batch 1000/1000: LR=8.96e-05, Loss=6.27e-02 BER=2.49e-02 FER=3.45e-01
Epoch 127 Train Time 75.3919267654419s

Training epoch 128, Batch 500/1000: LR=8.95e-05, Loss=6.32e-02 BER=2.52e-02 FER=3.47e-01
Training epoch 128, Batch 1000/1000: LR=8.95e-05, Loss=6.30e-02 BER=2.51e-02 FER=3.46e-01
Epoch 128 Train Time 75.39991784095764s

Training epoch 129, Batch 500/1000: LR=8.93e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.49e-01
Training epoch 129, Batch 1000/1000: LR=8.93e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.48e-01
Epoch 129 Train Time 75.52018880844116s

Training epoch 130, Batch 500/1000: LR=8.91e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.45e-01
Training epoch 130, Batch 1000/1000: LR=8.91e-05, Loss=6.25e-02 BER=2.48e-02 FER=3.44e-01
Epoch 130 Train Time 75.37592792510986s

Training epoch 131, Batch 500/1000: LR=8.90e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.45e-01
Training epoch 131, Batch 1000/1000: LR=8.90e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.45e-01
Epoch 131 Train Time 75.39191246032715s

Training epoch 132, Batch 500/1000: LR=8.88e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.44e-01
Training epoch 132, Batch 1000/1000: LR=8.88e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.45e-01
Epoch 132 Train Time 75.38390445709229s

Training epoch 133, Batch 500/1000: LR=8.86e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.44e-01
Training epoch 133, Batch 1000/1000: LR=8.86e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.45e-01
Epoch 133 Train Time 75.39993190765381s

Training epoch 134, Batch 500/1000: LR=8.85e-05, Loss=6.19e-02 BER=2.46e-02 FER=3.42e-01
Training epoch 134, Batch 1000/1000: LR=8.85e-05, Loss=6.21e-02 BER=2.47e-02 FER=3.42e-01
Epoch 134 Train Time 75.47987723350525s

Training epoch 135, Batch 500/1000: LR=8.83e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.43e-01
Training epoch 135, Batch 1000/1000: LR=8.83e-05, Loss=6.29e-02 BER=2.50e-02 FER=3.43e-01
Epoch 135 Train Time 75.5278811454773s

Training epoch 136, Batch 500/1000: LR=8.81e-05, Loss=6.35e-02 BER=2.52e-02 FER=3.47e-01
Training epoch 136, Batch 1000/1000: LR=8.81e-05, Loss=6.32e-02 BER=2.51e-02 FER=3.46e-01
Epoch 136 Train Time 75.44790363311768s

Training epoch 137, Batch 500/1000: LR=8.80e-05, Loss=6.28e-02 BER=2.49e-02 FER=3.45e-01
Training epoch 137, Batch 1000/1000: LR=8.80e-05, Loss=6.24e-02 BER=2.48e-02 FER=3.42e-01
Epoch 137 Train Time 75.4159049987793s

Training epoch 138, Batch 500/1000: LR=8.78e-05, Loss=6.22e-02 BER=2.47e-02 FER=3.40e-01
Training epoch 138, Batch 1000/1000: LR=8.78e-05, Loss=6.24e-02 BER=2.47e-02 FER=3.42e-01
Epoch 138 Train Time 75.44790506362915s

Training epoch 139, Batch 500/1000: LR=8.76e-05, Loss=6.23e-02 BER=2.48e-02 FER=3.43e-01
Training epoch 139, Batch 1000/1000: LR=8.76e-05, Loss=6.23e-02 BER=2.48e-02 FER=3.43e-01
Epoch 139 Train Time 75.43190455436707s

Training epoch 140, Batch 500/1000: LR=8.75e-05, Loss=6.30e-02 BER=2.51e-02 FER=3.45e-01
Training epoch 140, Batch 1000/1000: LR=8.75e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.44e-01
Epoch 140 Train Time 75.39990401268005s

Training epoch 141, Batch 500/1000: LR=8.73e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.41e-01
Training epoch 141, Batch 1000/1000: LR=8.73e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.43e-01
Epoch 141 Train Time 75.40793299674988s

Training epoch 142, Batch 500/1000: LR=8.71e-05, Loss=6.27e-02 BER=2.49e-02 FER=3.44e-01
Training epoch 142, Batch 1000/1000: LR=8.71e-05, Loss=6.25e-02 BER=2.49e-02 FER=3.43e-01
Epoch 142 Train Time 75.4158763885498s

Training epoch 143, Batch 500/1000: LR=8.69e-05, Loss=6.22e-02 BER=2.48e-02 FER=3.43e-01
Training epoch 143, Batch 1000/1000: LR=8.69e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.44e-01
Epoch 143 Train Time 75.4079053401947s

Training epoch 144, Batch 500/1000: LR=8.68e-05, Loss=6.26e-02 BER=2.50e-02 FER=3.43e-01
Training epoch 144, Batch 1000/1000: LR=8.68e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.43e-01
Epoch 144 Train Time 75.42392659187317s

Training epoch 145, Batch 500/1000: LR=8.66e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.45e-01
Training epoch 145, Batch 1000/1000: LR=8.66e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.43e-01
Epoch 145 Train Time 75.46388149261475s

Training epoch 146, Batch 500/1000: LR=8.64e-05, Loss=6.22e-02 BER=2.48e-02 FER=3.44e-01
Training epoch 146, Batch 1000/1000: LR=8.64e-05, Loss=6.24e-02 BER=2.49e-02 FER=3.44e-01
Epoch 146 Train Time 103.98603868484497s

Training epoch 147, Batch 500/1000: LR=8.62e-05, Loss=6.30e-02 BER=2.50e-02 FER=3.46e-01
Training epoch 147, Batch 1000/1000: LR=8.62e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.44e-01
Epoch 147 Train Time 76.3298499584198s

Training epoch 148, Batch 500/1000: LR=8.60e-05, Loss=6.33e-02 BER=2.52e-02 FER=3.46e-01
Training epoch 148, Batch 1000/1000: LR=8.60e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.45e-01
Epoch 148 Train Time 75.59978294372559s

Training epoch 149, Batch 500/1000: LR=8.59e-05, Loss=6.28e-02 BER=2.49e-02 FER=3.44e-01
Training epoch 149, Batch 1000/1000: LR=8.59e-05, Loss=6.21e-02 BER=2.47e-02 FER=3.41e-01
Epoch 149 Train Time 75.42759156227112s

Training epoch 150, Batch 500/1000: LR=8.57e-05, Loss=6.34e-02 BER=2.53e-02 FER=3.44e-01
Training epoch 150, Batch 1000/1000: LR=8.57e-05, Loss=6.31e-02 BER=2.52e-02 FER=3.44e-01
Epoch 150 Train Time 75.38915133476257s

Training epoch 151, Batch 500/1000: LR=8.55e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.44e-01
Training epoch 151, Batch 1000/1000: LR=8.55e-05, Loss=6.31e-02 BER=2.51e-02 FER=3.45e-01
Epoch 151 Train Time 75.38050079345703s

Training epoch 152, Batch 500/1000: LR=8.53e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.42e-01
Training epoch 152, Batch 1000/1000: LR=8.53e-05, Loss=6.28e-02 BER=2.51e-02 FER=3.44e-01
Epoch 152 Train Time 75.39397597312927s

Training epoch 153, Batch 500/1000: LR=8.51e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.43e-01
Training epoch 153, Batch 1000/1000: LR=8.51e-05, Loss=6.26e-02 BER=2.50e-02 FER=3.43e-01
Epoch 153 Train Time 75.34099388122559s

Training epoch 154, Batch 500/1000: LR=8.49e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.43e-01
Training epoch 154, Batch 1000/1000: LR=8.49e-05, Loss=6.28e-02 BER=2.50e-02 FER=3.43e-01
Epoch 154 Train Time 75.47880363464355s

Training epoch 155, Batch 500/1000: LR=8.48e-05, Loss=6.29e-02 BER=2.51e-02 FER=3.44e-01
Training epoch 155, Batch 1000/1000: LR=8.48e-05, Loss=6.27e-02 BER=2.49e-02 FER=3.44e-01
Epoch 155 Train Time 75.36088991165161s

Training epoch 156, Batch 500/1000: LR=8.46e-05, Loss=6.22e-02 BER=2.47e-02 FER=3.40e-01
Training epoch 156, Batch 1000/1000: LR=8.46e-05, Loss=6.23e-02 BER=2.48e-02 FER=3.41e-01
Epoch 156 Train Time 75.34660649299622s

Training epoch 157, Batch 500/1000: LR=8.44e-05, Loss=6.30e-02 BER=2.51e-02 FER=3.46e-01
Training epoch 157, Batch 1000/1000: LR=8.44e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.44e-01
Epoch 157 Train Time 75.28690075874329s

Training epoch 158, Batch 500/1000: LR=8.42e-05, Loss=6.20e-02 BER=2.47e-02 FER=3.41e-01
Training epoch 158, Batch 1000/1000: LR=8.42e-05, Loss=6.19e-02 BER=2.47e-02 FER=3.40e-01
Epoch 158 Train Time 75.37218618392944s

Training epoch 159, Batch 500/1000: LR=8.40e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.45e-01
Training epoch 159, Batch 1000/1000: LR=8.40e-05, Loss=6.26e-02 BER=2.49e-02 FER=3.44e-01
Epoch 159 Train Time 75.45863342285156s

Training epoch 160, Batch 500/1000: LR=8.38e-05, Loss=6.17e-02 BER=2.46e-02 FER=3.38e-01
Training epoch 160, Batch 1000/1000: LR=8.38e-05, Loss=6.20e-02 BER=2.47e-02 FER=3.40e-01
Epoch 160 Train Time 75.39225959777832s

Training epoch 161, Batch 500/1000: LR=8.36e-05, Loss=6.24e-02 BER=2.48e-02 FER=3.43e-01
Training epoch 161, Batch 1000/1000: LR=8.36e-05, Loss=6.28e-02 BER=2.49e-02 FER=3.44e-01
Epoch 161 Train Time 75.52072334289551s

Training epoch 162, Batch 500/1000: LR=8.34e-05, Loss=6.27e-02 BER=2.50e-02 FER=3.43e-01
Training epoch 162, Batch 1000/1000: LR=8.34e-05, Loss=6.23e-02 BER=2.48e-02 FER=3.40e-01
Epoch 162 Train Time 75.99984049797058s

Training epoch 163, Batch 500/1000: LR=8.32e-05, Loss=6.19e-02 BER=2.47e-02 FER=3.41e-01
Training epoch 163, Batch 1000/1000: LR=8.32e-05, Loss=6.24e-02 BER=2.48e-02 FER=3.42e-01
Epoch 163 Train Time 78.78776407241821s

Training epoch 164, Batch 500/1000: LR=8.30e-05, Loss=6.22e-02 BER=2.48e-02 FER=3.42e-01
Training epoch 164, Batch 1000/1000: LR=8.30e-05, Loss=6.21e-02 BER=2.47e-02 FER=3.41e-01
Epoch 164 Train Time 77.29113554954529s

Training epoch 165, Batch 500/1000: LR=8.28e-05, Loss=6.18e-02 BER=2.46e-02 FER=3.39e-01
Training epoch 165, Batch 1000/1000: LR=8.28e-05, Loss=6.21e-02 BER=2.47e-02 FER=3.41e-01
Epoch 165 Train Time 77.70950412750244s

Training epoch 166, Batch 500/1000: LR=8.26e-05, Loss=6.21e-02 BER=2.47e-02 FER=3.41e-01
Training epoch 166, Batch 1000/1000: LR=8.26e-05, Loss=6.25e-02 BER=2.49e-02 FER=3.43e-01
Epoch 166 Train Time 84.08579587936401s

Training epoch 167, Batch 500/1000: LR=8.25e-05, Loss=6.27e-02 BER=2.49e-02 FER=3.44e-01
