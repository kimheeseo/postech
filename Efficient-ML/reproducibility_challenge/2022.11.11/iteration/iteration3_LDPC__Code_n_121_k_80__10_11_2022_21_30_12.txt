Path to model/logs: Results_ECCT\LDPC__Code_n_121_k_80__10_11_2022_21_30_12
Namespace(epochs=600, workers=0, lr=0.0001, gpus='0', batch_size=128, test_batch_size=512, seed=42, code_type='LDPC', code_k=80, code_n=121, standardize=False, N_dec=6, d_model=32, h=8, code=<__main__.Code object at 0x0000019205552470>, path='Results_ECCT\\LDPC__Code_n_121_k_80__10_11_2022_21_30_12')
Self-Attention Sparsity Ratio=78.06%, Self-Attention Complexity Ratio=10.97%
Mask:
 tensor([[[[False,  True,  True,  ...,  True,  True,  True],
          [ True, False,  True,  ...,  True,  True,  True],
          [ True,  True, False,  ...,  True,  True,  True],
          ...,
          [ True,  True,  True,  ..., False,  True,  True],
          [ True,  True,  True,  ...,  True, False,  True],
          [ True,  True,  True,  ...,  True,  True, False]]]])
ECC_Transformer(
  (decoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
      (5): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=32, out_features=32, bias=True)
            (1): Linear(in_features=32, out_features=32, bias=True)
            (2): Linear(in_features=32, out_features=32, bias=True)
            (3): Linear(in_features=32, out_features=32, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=128, bias=True)
          (w_2): Linear(in_features=128, out_features=32, bias=True)
          (dropout): Dropout(p=0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (oned_final_embed): Sequential(
    (0): Linear(in_features=32, out_features=1, bias=True)
  )
  (out_fc): Linear(in_features=165, out_features=121, bias=True)
)
# of Parameters: 101751
Training epoch 1, Batch 500/1000: LR=1.00e-04, Loss=1.87e-01 BER=6.08e-02 FER=8.70e-01
Training epoch 1, Batch 1000/1000: LR=1.00e-04, Loss=1.60e-01 BER=4.68e-02 FER=8.58e-01
Epoch 1 Train Time 88.48051452636719s


Test Loss 4: 1.43e-01 5: 9.35e-02 6: 5.41e-02
Test FER 4: 9.84e-01 5: 9.18e-01 6: 7.32e-01
Test BER 4: 3.42e-02 5: 2.04e-02 6: 1.08e-02
Test -ln(BER) 4: 3.38e+00 5: 3.89e+00 6: 4.53e+00
# of testing samples: [100352.0, 100352.0, 100352.0]
 Test Time 109.7563955783844 s

Training epoch 2, Batch 500/1000: LR=1.00e-04, Loss=1.29e-01 BER=3.27e-02 FER=8.48e-01
Training epoch 2, Batch 1000/1000: LR=1.00e-04, Loss=1.27e-01 BER=3.27e-02 FER=8.47e-01
Epoch 2 Train Time 86.30033254623413s

Training epoch 3, Batch 500/1000: LR=1.00e-04, Loss=1.16e-01 BER=3.24e-02 FER=8.24e-01
Training epoch 3, Batch 1000/1000: LR=1.00e-04, Loss=1.13e-01 BER=3.19e-02 FER=7.83e-01
Epoch 3 Train Time 86.39475345611572s

Training epoch 4, Batch 500/1000: LR=1.00e-04, Loss=1.02e-01 BER=3.03e-02 FER=6.74e-01
Training epoch 4, Batch 1000/1000: LR=1.00e-04, Loss=1.00e-01 BER=2.98e-02 FER=6.53e-01
Epoch 4 Train Time 86.83462309837341s

Training epoch 5, Batch 500/1000: LR=1.00e-04, Loss=9.47e-02 BER=2.88e-02 FER=6.08e-01
Training epoch 5, Batch 1000/1000: LR=1.00e-04, Loss=9.30e-02 BER=2.85e-02 FER=5.97e-01
Epoch 5 Train Time 86.39164328575134s

Training epoch 6, Batch 500/1000: LR=1.00e-04, Loss=9.00e-02 BER=2.80e-02 FER=5.75e-01
Training epoch 6, Batch 1000/1000: LR=1.00e-04, Loss=8.87e-02 BER=2.78e-02 FER=5.70e-01
Epoch 6 Train Time 86.51198196411133s

Training epoch 7, Batch 500/1000: LR=1.00e-04, Loss=8.45e-02 BER=2.72e-02 FER=5.57e-01
Training epoch 7, Batch 1000/1000: LR=1.00e-04, Loss=8.28e-02 BER=2.70e-02 FER=5.52e-01
Epoch 7 Train Time 86.59430170059204s

Training epoch 8, Batch 500/1000: LR=1.00e-04, Loss=7.74e-02 BER=2.61e-02 FER=5.35e-01
Training epoch 8, Batch 1000/1000: LR=1.00e-04, Loss=7.58e-02 BER=2.59e-02 FER=5.30e-01
Epoch 8 Train Time 86.42884659767151s

Training epoch 9, Batch 500/1000: LR=1.00e-04, Loss=7.02e-02 BER=2.47e-02 FER=5.04e-01
Training epoch 9, Batch 1000/1000: LR=1.00e-04, Loss=6.81e-02 BER=2.41e-02 FER=4.92e-01
Epoch 9 Train Time 86.40508556365967s

Training epoch 10, Batch 500/1000: LR=9.99e-05, Loss=6.30e-02 BER=2.26e-02 FER=4.58e-01
Training epoch 10, Batch 1000/1000: LR=9.99e-05, Loss=6.13e-02 BER=2.20e-02 FER=4.49e-01
Epoch 10 Train Time 86.34773850440979s

Training epoch 11, Batch 500/1000: LR=9.99e-05, Loss=5.59e-02 BER=2.03e-02 FER=4.17e-01
Training epoch 11, Batch 1000/1000: LR=9.99e-05, Loss=5.48e-02 BER=1.99e-02 FER=4.12e-01
Epoch 11 Train Time 86.23797726631165s

Training epoch 12, Batch 500/1000: LR=9.99e-05, Loss=5.13e-02 BER=1.86e-02 FER=3.91e-01
Training epoch 12, Batch 1000/1000: LR=9.99e-05, Loss=5.07e-02 BER=1.83e-02 FER=3.87e-01
Epoch 12 Train Time 86.32622528076172s

Training epoch 13, Batch 500/1000: LR=9.99e-05, Loss=4.85e-02 BER=1.75e-02 FER=3.69e-01
Training epoch 13, Batch 1000/1000: LR=9.99e-05, Loss=4.73e-02 BER=1.71e-02 FER=3.62e-01
Epoch 13 Train Time 86.14339876174927s

Training epoch 14, Batch 500/1000: LR=9.99e-05, Loss=4.51e-02 BER=1.63e-02 FER=3.47e-01
Training epoch 14, Batch 1000/1000: LR=9.99e-05, Loss=4.45e-02 BER=1.61e-02 FER=3.42e-01
Epoch 14 Train Time 86.12737965583801s

Training epoch 15, Batch 500/1000: LR=9.99e-05, Loss=4.23e-02 BER=1.54e-02 FER=3.27e-01
Training epoch 15, Batch 1000/1000: LR=9.99e-05, Loss=4.21e-02 BER=1.53e-02 FER=3.22e-01
Epoch 15 Train Time 86.13148665428162s

Training epoch 16, Batch 500/1000: LR=9.98e-05, Loss=4.04e-02 BER=1.47e-02 FER=3.06e-01
Training epoch 16, Batch 1000/1000: LR=9.98e-05, Loss=3.98e-02 BER=1.45e-02 FER=3.03e-01
Epoch 16 Train Time 86.24915146827698s

Training epoch 17, Batch 500/1000: LR=9.98e-05, Loss=3.88e-02 BER=1.41e-02 FER=2.94e-01
Training epoch 17, Batch 1000/1000: LR=9.98e-05, Loss=3.80e-02 BER=1.39e-02 FER=2.90e-01
Epoch 17 Train Time 86.29218292236328s

Training epoch 18, Batch 500/1000: LR=9.98e-05, Loss=3.71e-02 BER=1.36e-02 FER=2.83e-01
Training epoch 18, Batch 1000/1000: LR=9.98e-05, Loss=3.66e-02 BER=1.35e-02 FER=2.79e-01
Epoch 18 Train Time 86.0763008594513s

Training epoch 19, Batch 500/1000: LR=9.98e-05, Loss=3.58e-02 BER=1.32e-02 FER=2.69e-01
Training epoch 19, Batch 1000/1000: LR=9.98e-05, Loss=3.53e-02 BER=1.31e-02 FER=2.67e-01
Epoch 19 Train Time 86.22659611701965s

Training epoch 20, Batch 500/1000: LR=9.98e-05, Loss=3.44e-02 BER=1.28e-02 FER=2.62e-01
Training epoch 20, Batch 1000/1000: LR=9.98e-05, Loss=3.41e-02 BER=1.27e-02 FER=2.60e-01
Epoch 20 Train Time 86.10603713989258s

Training epoch 21, Batch 500/1000: LR=9.97e-05, Loss=3.36e-02 BER=1.25e-02 FER=2.54e-01
Training epoch 21, Batch 1000/1000: LR=9.97e-05, Loss=3.34e-02 BER=1.24e-02 FER=2.51e-01
Epoch 21 Train Time 86.12632703781128s

Training epoch 22, Batch 500/1000: LR=9.97e-05, Loss=3.31e-02 BER=1.24e-02 FER=2.48e-01
Training epoch 22, Batch 1000/1000: LR=9.97e-05, Loss=3.29e-02 BER=1.23e-02 FER=2.46e-01
Epoch 22 Train Time 86.11379432678223s

Training epoch 23, Batch 500/1000: LR=9.97e-05, Loss=3.18e-02 BER=1.20e-02 FER=2.40e-01
Training epoch 23, Batch 1000/1000: LR=9.97e-05, Loss=3.19e-02 BER=1.20e-02 FER=2.39e-01
Epoch 23 Train Time 86.31733441352844s

Training epoch 24, Batch 500/1000: LR=9.96e-05, Loss=3.17e-02 BER=1.20e-02 FER=2.36e-01
Training epoch 24, Batch 1000/1000: LR=9.96e-05, Loss=3.17e-02 BER=1.20e-02 FER=2.36e-01
Epoch 24 Train Time 115.17128014564514s

Training epoch 25, Batch 500/1000: LR=9.96e-05, Loss=3.12e-02 BER=1.19e-02 FER=2.34e-01
Training epoch 25, Batch 1000/1000: LR=9.96e-05, Loss=3.10e-02 BER=1.17e-02 FER=2.31e-01
Epoch 25 Train Time 86.5383186340332s

Training epoch 26, Batch 500/1000: LR=9.96e-05, Loss=3.05e-02 BER=1.16e-02 FER=2.28e-01
Training epoch 26, Batch 1000/1000: LR=9.96e-05, Loss=3.06e-02 BER=1.16e-02 FER=2.27e-01
Epoch 26 Train Time 86.32273554801941s

Training epoch 27, Batch 500/1000: LR=9.95e-05, Loss=3.06e-02 BER=1.16e-02 FER=2.26e-01
Training epoch 27, Batch 1000/1000: LR=9.95e-05, Loss=3.07e-02 BER=1.16e-02 FER=2.26e-01
Epoch 27 Train Time 86.33541440963745s

Training epoch 28, Batch 500/1000: LR=9.95e-05, Loss=3.04e-02 BER=1.15e-02 FER=2.24e-01
Training epoch 28, Batch 1000/1000: LR=9.95e-05, Loss=3.00e-02 BER=1.14e-02 FER=2.21e-01
Epoch 28 Train Time 86.446937084198s

Training epoch 29, Batch 500/1000: LR=9.95e-05, Loss=3.00e-02 BER=1.13e-02 FER=2.20e-01
Training epoch 29, Batch 1000/1000: LR=9.95e-05, Loss=2.99e-02 BER=1.14e-02 FER=2.20e-01
Epoch 29 Train Time 86.3048644065857s

Training epoch 30, Batch 500/1000: LR=9.94e-05, Loss=2.97e-02 BER=1.13e-02 FER=2.17e-01
Training epoch 30, Batch 1000/1000: LR=9.94e-05, Loss=2.95e-02 BER=1.12e-02 FER=2.17e-01
Epoch 30 Train Time 86.45632553100586s

Training epoch 31, Batch 500/1000: LR=9.94e-05, Loss=2.88e-02 BER=1.10e-02 FER=2.11e-01
Training epoch 31, Batch 1000/1000: LR=9.94e-05, Loss=2.90e-02 BER=1.10e-02 FER=2.13e-01
Epoch 31 Train Time 86.2795901298523s

Training epoch 32, Batch 500/1000: LR=9.93e-05, Loss=2.89e-02 BER=1.10e-02 FER=2.12e-01
Training epoch 32, Batch 1000/1000: LR=9.93e-05, Loss=2.90e-02 BER=1.10e-02 FER=2.12e-01
Epoch 32 Train Time 86.48280143737793s

Training epoch 33, Batch 500/1000: LR=9.93e-05, Loss=2.93e-02 BER=1.12e-02 FER=2.13e-01
Training epoch 33, Batch 1000/1000: LR=9.93e-05, Loss=2.90e-02 BER=1.11e-02 FER=2.12e-01
Epoch 33 Train Time 86.24420070648193s

Training epoch 34, Batch 500/1000: LR=9.93e-05, Loss=2.89e-02 BER=1.10e-02 FER=2.12e-01
Training epoch 34, Batch 1000/1000: LR=9.93e-05, Loss=2.89e-02 BER=1.10e-02 FER=2.12e-01
Epoch 34 Train Time 86.17046308517456s

Training epoch 35, Batch 500/1000: LR=9.92e-05, Loss=2.85e-02 BER=1.09e-02 FER=2.09e-01
Training epoch 35, Batch 1000/1000: LR=9.92e-05, Loss=2.86e-02 BER=1.09e-02 FER=2.09e-01
Epoch 35 Train Time 86.23919415473938s

Training epoch 36, Batch 500/1000: LR=9.92e-05, Loss=2.80e-02 BER=1.06e-02 FER=2.07e-01
Training epoch 36, Batch 1000/1000: LR=9.92e-05, Loss=2.84e-02 BER=1.08e-02 FER=2.08e-01
Epoch 36 Train Time 86.01058650016785s

Training epoch 37, Batch 500/1000: LR=9.91e-05, Loss=2.84e-02 BER=1.08e-02 FER=2.08e-01
Training epoch 37, Batch 1000/1000: LR=9.91e-05, Loss=2.84e-02 BER=1.08e-02 FER=2.06e-01
Epoch 37 Train Time 86.29722428321838s

Training epoch 38, Batch 500/1000: LR=9.91e-05, Loss=2.83e-02 BER=1.08e-02 FER=2.05e-01
Training epoch 38, Batch 1000/1000: LR=9.91e-05, Loss=2.84e-02 BER=1.08e-02 FER=2.05e-01
Epoch 38 Train Time 85.92676830291748s

Training epoch 39, Batch 500/1000: LR=9.90e-05, Loss=2.85e-02 BER=1.09e-02 FER=2.05e-01
Training epoch 39, Batch 1000/1000: LR=9.90e-05, Loss=2.83e-02 BER=1.08e-02 FER=2.04e-01
Epoch 39 Train Time 85.90964269638062s

Training epoch 40, Batch 500/1000: LR=9.90e-05, Loss=2.85e-02 BER=1.09e-02 FER=2.05e-01
Training epoch 40, Batch 1000/1000: LR=9.90e-05, Loss=2.82e-02 BER=1.08e-02 FER=2.03e-01
Epoch 40 Train Time 86.05385494232178s

Training epoch 41, Batch 500/1000: LR=9.89e-05, Loss=2.83e-02 BER=1.08e-02 FER=2.03e-01
Training epoch 41, Batch 1000/1000: LR=9.89e-05, Loss=2.81e-02 BER=1.07e-02 FER=2.02e-01
Epoch 41 Train Time 86.07396721839905s

Training epoch 42, Batch 500/1000: LR=9.89e-05, Loss=2.75e-02 BER=1.06e-02 FER=1.99e-01
Training epoch 42, Batch 1000/1000: LR=9.89e-05, Loss=2.76e-02 BER=1.06e-02 FER=2.00e-01
Epoch 42 Train Time 86.06240487098694s

Training epoch 43, Batch 500/1000: LR=9.88e-05, Loss=2.80e-02 BER=1.07e-02 FER=2.01e-01
Training epoch 43, Batch 1000/1000: LR=9.88e-05, Loss=2.78e-02 BER=1.07e-02 FER=2.00e-01
Epoch 43 Train Time 86.04986691474915s

Training epoch 44, Batch 500/1000: LR=9.88e-05, Loss=2.77e-02 BER=1.07e-02 FER=2.00e-01
Training epoch 44, Batch 1000/1000: LR=9.88e-05, Loss=2.78e-02 BER=1.06e-02 FER=2.00e-01
Epoch 44 Train Time 85.90109610557556s

Training epoch 45, Batch 500/1000: LR=9.87e-05, Loss=2.76e-02 BER=1.06e-02 FER=1.96e-01
Training epoch 45, Batch 1000/1000: LR=9.87e-05, Loss=2.75e-02 BER=1.05e-02 FER=1.96e-01
Epoch 45 Train Time 143.02756428718567s

Training epoch 46, Batch 500/1000: LR=9.86e-05, Loss=2.74e-02 BER=1.05e-02 FER=1.96e-01
Training epoch 46, Batch 1000/1000: LR=9.86e-05, Loss=2.74e-02 BER=1.05e-02 FER=1.96e-01
Epoch 46 Train Time 86.39871263504028s

Training epoch 47, Batch 500/1000: LR=9.86e-05, Loss=2.77e-02 BER=1.06e-02 FER=1.99e-01
Training epoch 47, Batch 1000/1000: LR=9.86e-05, Loss=2.76e-02 BER=1.05e-02 FER=1.98e-01
Epoch 47 Train Time 86.05970478057861s

Training epoch 48, Batch 500/1000: LR=9.85e-05, Loss=2.72e-02 BER=1.04e-02 FER=1.95e-01
Training epoch 48, Batch 1000/1000: LR=9.85e-05, Loss=2.73e-02 BER=1.04e-02 FER=1.95e-01
Epoch 48 Train Time 85.89689421653748s

Training epoch 49, Batch 500/1000: LR=9.84e-05, Loss=2.71e-02 BER=1.04e-02 FER=1.96e-01
Training epoch 49, Batch 1000/1000: LR=9.84e-05, Loss=2.71e-02 BER=1.04e-02 FER=1.96e-01
Epoch 49 Train Time 86.21380305290222s

Training epoch 50, Batch 500/1000: LR=9.84e-05, Loss=2.73e-02 BER=1.05e-02 FER=1.95e-01
Training epoch 50, Batch 1000/1000: LR=9.84e-05, Loss=2.74e-02 BER=1.05e-02 FER=1.94e-01
Epoch 50 Train Time 86.04819083213806s

Training epoch 51, Batch 500/1000: LR=9.83e-05, Loss=2.74e-02 BER=1.05e-02 FER=1.94e-01
Training epoch 51, Batch 1000/1000: LR=9.83e-05, Loss=2.71e-02 BER=1.04e-02 FER=1.93e-01
Epoch 51 Train Time 85.90186858177185s

Training epoch 52, Batch 500/1000: LR=9.82e-05, Loss=2.73e-02 BER=1.05e-02 FER=1.95e-01
Training epoch 52, Batch 1000/1000: LR=9.82e-05, Loss=2.72e-02 BER=1.04e-02 FER=1.94e-01
Epoch 52 Train Time 86.18590664863586s

Training epoch 53, Batch 500/1000: LR=9.82e-05, Loss=2.72e-02 BER=1.04e-02 FER=1.93e-01
Training epoch 53, Batch 1000/1000: LR=9.82e-05, Loss=2.70e-02 BER=1.03e-02 FER=1.93e-01
Epoch 53 Train Time 85.9780535697937s

Training epoch 54, Batch 500/1000: LR=9.81e-05, Loss=2.70e-02 BER=1.03e-02 FER=1.93e-01
Training epoch 54, Batch 1000/1000: LR=9.81e-05, Loss=2.69e-02 BER=1.03e-02 FER=1.92e-01
Epoch 54 Train Time 86.18814706802368s

Training epoch 55, Batch 500/1000: LR=9.80e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.91e-01
Training epoch 55, Batch 1000/1000: LR=9.80e-05, Loss=2.67e-02 BER=1.02e-02 FER=1.90e-01
Epoch 55 Train Time 85.99243569374084s

Training epoch 56, Batch 500/1000: LR=9.80e-05, Loss=2.72e-02 BER=1.04e-02 FER=1.93e-01
Training epoch 56, Batch 1000/1000: LR=9.80e-05, Loss=2.72e-02 BER=1.04e-02 FER=1.93e-01
Epoch 56 Train Time 86.24696826934814s

Training epoch 57, Batch 500/1000: LR=9.79e-05, Loss=2.67e-02 BER=1.03e-02 FER=1.91e-01
Training epoch 57, Batch 1000/1000: LR=9.79e-05, Loss=2.67e-02 BER=1.03e-02 FER=1.91e-01
Epoch 57 Train Time 85.90458416938782s

Training epoch 58, Batch 500/1000: LR=9.78e-05, Loss=2.69e-02 BER=1.04e-02 FER=1.91e-01
Training epoch 58, Batch 1000/1000: LR=9.78e-05, Loss=2.68e-02 BER=1.03e-02 FER=1.91e-01
Epoch 58 Train Time 85.91273903846741s

Training epoch 59, Batch 500/1000: LR=9.77e-05, Loss=2.68e-02 BER=1.03e-02 FER=1.91e-01
Training epoch 59, Batch 1000/1000: LR=9.77e-05, Loss=2.68e-02 BER=1.03e-02 FER=1.90e-01
Epoch 59 Train Time 85.8812165260315s

Training epoch 60, Batch 500/1000: LR=9.77e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.89e-01
Training epoch 60, Batch 1000/1000: LR=9.77e-05, Loss=2.65e-02 BER=1.02e-02 FER=1.88e-01
Epoch 60 Train Time 85.89969062805176s

Training epoch 61, Batch 500/1000: LR=9.76e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.89e-01
Training epoch 61, Batch 1000/1000: LR=9.76e-05, Loss=2.67e-02 BER=1.03e-02 FER=1.90e-01
Epoch 61 Train Time 86.17799162864685s

Training epoch 62, Batch 500/1000: LR=9.75e-05, Loss=2.66e-02 BER=1.01e-02 FER=1.89e-01
Training epoch 62, Batch 1000/1000: LR=9.75e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.88e-01
Epoch 62 Train Time 85.86363124847412s

Training epoch 63, Batch 500/1000: LR=9.74e-05, Loss=2.70e-02 BER=1.04e-02 FER=1.89e-01
Training epoch 63, Batch 1000/1000: LR=9.74e-05, Loss=2.72e-02 BER=1.04e-02 FER=1.91e-01
Epoch 63 Train Time 85.91819357872009s

Training epoch 64, Batch 500/1000: LR=9.73e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.87e-01
Training epoch 64, Batch 1000/1000: LR=9.73e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.87e-01
Epoch 64 Train Time 85.92814183235168s

Training epoch 65, Batch 500/1000: LR=9.72e-05, Loss=2.70e-02 BER=1.04e-02 FER=1.90e-01
Training epoch 65, Batch 1000/1000: LR=9.72e-05, Loss=2.67e-02 BER=1.03e-02 FER=1.89e-01
Epoch 65 Train Time 85.91212439537048s

Training epoch 66, Batch 500/1000: LR=9.72e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.86e-01
Training epoch 66, Batch 1000/1000: LR=9.72e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.86e-01
Epoch 66 Train Time 107.42131900787354s

Training epoch 67, Batch 500/1000: LR=9.71e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.88e-01
Training epoch 67, Batch 1000/1000: LR=9.71e-05, Loss=2.68e-02 BER=1.03e-02 FER=1.89e-01
Epoch 67 Train Time 86.47100305557251s

Training epoch 68, Batch 500/1000: LR=9.70e-05, Loss=2.64e-02 BER=1.01e-02 FER=1.88e-01
Training epoch 68, Batch 1000/1000: LR=9.70e-05, Loss=2.65e-02 BER=1.02e-02 FER=1.88e-01
Epoch 68 Train Time 86.30745005607605s

Training epoch 69, Batch 500/1000: LR=9.69e-05, Loss=2.67e-02 BER=1.03e-02 FER=1.89e-01
Training epoch 69, Batch 1000/1000: LR=9.69e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.87e-01
Epoch 69 Train Time 86.21194005012512s

Training epoch 70, Batch 500/1000: LR=9.68e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.86e-01
Training epoch 70, Batch 1000/1000: LR=9.68e-05, Loss=2.64e-02 BER=1.01e-02 FER=1.85e-01
Epoch 70 Train Time 86.30733013153076s

Training epoch 71, Batch 500/1000: LR=9.67e-05, Loss=2.68e-02 BER=1.03e-02 FER=1.88e-01
Training epoch 71, Batch 1000/1000: LR=9.67e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.86e-01
Epoch 71 Train Time 86.17269587516785s

Training epoch 72, Batch 500/1000: LR=9.66e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.86e-01
Training epoch 72, Batch 1000/1000: LR=9.66e-05, Loss=2.62e-02 BER=1.01e-02 FER=1.86e-01
Epoch 72 Train Time 86.18347978591919s

Training epoch 73, Batch 500/1000: LR=9.65e-05, Loss=2.67e-02 BER=1.03e-02 FER=1.87e-01
Training epoch 73, Batch 1000/1000: LR=9.65e-05, Loss=2.67e-02 BER=1.02e-02 FER=1.87e-01
Epoch 73 Train Time 86.3125069141388s

Training epoch 74, Batch 500/1000: LR=9.64e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.87e-01
Training epoch 74, Batch 1000/1000: LR=9.64e-05, Loss=2.64e-02 BER=1.01e-02 FER=1.87e-01
Epoch 74 Train Time 86.20520639419556s

Training epoch 75, Batch 500/1000: LR=9.63e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.85e-01
Training epoch 75, Batch 1000/1000: LR=9.63e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.85e-01
Epoch 75 Train Time 85.91717290878296s

Training epoch 76, Batch 500/1000: LR=9.62e-05, Loss=2.67e-02 BER=1.02e-02 FER=1.87e-01
Training epoch 76, Batch 1000/1000: LR=9.62e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.86e-01
Epoch 76 Train Time 86.01086640357971s

Training epoch 77, Batch 500/1000: LR=9.61e-05, Loss=2.67e-02 BER=1.02e-02 FER=1.86e-01
Training epoch 77, Batch 1000/1000: LR=9.61e-05, Loss=2.68e-02 BER=1.03e-02 FER=1.87e-01
Epoch 77 Train Time 85.89901351928711s

Training epoch 78, Batch 500/1000: LR=9.60e-05, Loss=2.60e-02 BER=1.00e-02 FER=1.83e-01
Training epoch 78, Batch 1000/1000: LR=9.60e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.83e-01
Epoch 78 Train Time 85.85896325111389s

Training epoch 79, Batch 500/1000: LR=9.59e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.86e-01
Training epoch 79, Batch 1000/1000: LR=9.59e-05, Loss=2.65e-02 BER=1.02e-02 FER=1.85e-01
Epoch 79 Train Time 86.05963802337646s

Training epoch 80, Batch 500/1000: LR=9.58e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.84e-01
Training epoch 80, Batch 1000/1000: LR=9.58e-05, Loss=2.64e-02 BER=1.02e-02 FER=1.85e-01
Epoch 80 Train Time 85.88815927505493s

Training epoch 81, Batch 500/1000: LR=9.57e-05, Loss=2.66e-02 BER=1.02e-02 FER=1.85e-01
Training epoch 81, Batch 1000/1000: LR=9.57e-05, Loss=2.64e-02 BER=1.02e-02 FER=1.86e-01
Epoch 81 Train Time 85.93344044685364s

Training epoch 82, Batch 500/1000: LR=9.56e-05, Loss=2.65e-02 BER=1.02e-02 FER=1.86e-01
Training epoch 82, Batch 1000/1000: LR=9.56e-05, Loss=2.62e-02 BER=1.01e-02 FER=1.84e-01
Epoch 82 Train Time 85.86868405342102s

Training epoch 83, Batch 500/1000: LR=9.55e-05, Loss=2.60e-02 BER=9.99e-03 FER=1.83e-01
Training epoch 83, Batch 1000/1000: LR=9.55e-05, Loss=2.58e-02 BER=9.93e-03 FER=1.81e-01
Epoch 83 Train Time 85.88446927070618s

Training epoch 84, Batch 500/1000: LR=9.54e-05, Loss=2.59e-02 BER=9.93e-03 FER=1.81e-01
Training epoch 84, Batch 1000/1000: LR=9.54e-05, Loss=2.60e-02 BER=9.99e-03 FER=1.82e-01
Epoch 84 Train Time 86.00433588027954s

Training epoch 85, Batch 500/1000: LR=9.53e-05, Loss=2.56e-02 BER=9.89e-03 FER=1.81e-01
Training epoch 85, Batch 1000/1000: LR=9.53e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.84e-01
Epoch 85 Train Time 85.92916965484619s

Training epoch 86, Batch 500/1000: LR=9.52e-05, Loss=2.60e-02 BER=1.01e-02 FER=1.83e-01
Training epoch 86, Batch 1000/1000: LR=9.52e-05, Loss=2.63e-02 BER=1.02e-02 FER=1.83e-01
Epoch 86 Train Time 86.02514147758484s

Training epoch 87, Batch 500/1000: LR=9.51e-05, Loss=2.58e-02 BER=9.90e-03 FER=1.81e-01
Training epoch 87, Batch 1000/1000: LR=9.51e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.82e-01
Epoch 87 Train Time 85.90766620635986s

Training epoch 88, Batch 500/1000: LR=9.50e-05, Loss=2.60e-02 BER=9.98e-03 FER=1.81e-01
Training epoch 88, Batch 1000/1000: LR=9.50e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.82e-01
Epoch 88 Train Time 85.91258645057678s

Training epoch 89, Batch 500/1000: LR=9.48e-05, Loss=2.60e-02 BER=1.00e-02 FER=1.83e-01
Training epoch 89, Batch 1000/1000: LR=9.48e-05, Loss=2.60e-02 BER=1.00e-02 FER=1.83e-01
Epoch 89 Train Time 85.8860821723938s

Training epoch 90, Batch 500/1000: LR=9.47e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.83e-01
Training epoch 90, Batch 1000/1000: LR=9.47e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.82e-01
Epoch 90 Train Time 85.90458106994629s

Training epoch 91, Batch 500/1000: LR=9.46e-05, Loss=2.60e-02 BER=1.00e-02 FER=1.83e-01
Training epoch 91, Batch 1000/1000: LR=9.46e-05, Loss=2.59e-02 BER=1.00e-02 FER=1.82e-01
Epoch 91 Train Time 85.95502233505249s

Training epoch 92, Batch 500/1000: LR=9.45e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.83e-01
Training epoch 92, Batch 1000/1000: LR=9.45e-05, Loss=2.63e-02 BER=1.01e-02 FER=1.83e-01
Epoch 92 Train Time 85.8921172618866s

Training epoch 93, Batch 500/1000: LR=9.44e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.83e-01
Training epoch 93, Batch 1000/1000: LR=9.44e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.82e-01
Epoch 93 Train Time 85.93173289299011s

Training epoch 94, Batch 500/1000: LR=9.42e-05, Loss=2.64e-02 BER=1.02e-02 FER=1.84e-01
Training epoch 94, Batch 1000/1000: LR=9.42e-05, Loss=2.60e-02 BER=1.00e-02 FER=1.82e-01
Epoch 94 Train Time 85.92042827606201s

Training epoch 95, Batch 500/1000: LR=9.41e-05, Loss=2.56e-02 BER=9.85e-03 FER=1.81e-01
Training epoch 95, Batch 1000/1000: LR=9.41e-05, Loss=2.60e-02 BER=9.99e-03 FER=1.82e-01
Epoch 95 Train Time 85.99995589256287s

Training epoch 96, Batch 500/1000: LR=9.40e-05, Loss=2.54e-02 BER=9.79e-03 FER=1.79e-01
Training epoch 96, Batch 1000/1000: LR=9.40e-05, Loss=2.58e-02 BER=9.95e-03 FER=1.81e-01
Epoch 96 Train Time 105.27580094337463s

Training epoch 97, Batch 500/1000: LR=9.39e-05, Loss=2.57e-02 BER=9.89e-03 FER=1.81e-01
Training epoch 97, Batch 1000/1000: LR=9.39e-05, Loss=2.59e-02 BER=1.00e-02 FER=1.81e-01
Epoch 97 Train Time 86.43815231323242s

Training epoch 98, Batch 500/1000: LR=9.38e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.83e-01
Training epoch 98, Batch 1000/1000: LR=9.38e-05, Loss=2.58e-02 BER=9.96e-03 FER=1.81e-01
Epoch 98 Train Time 86.18000435829163s

Training epoch 99, Batch 500/1000: LR=9.36e-05, Loss=2.62e-02 BER=1.01e-02 FER=1.83e-01
Training epoch 99, Batch 1000/1000: LR=9.36e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.82e-01
Epoch 99 Train Time 86.24465990066528s

Training epoch 100, Batch 500/1000: LR=9.35e-05, Loss=2.62e-02 BER=1.01e-02 FER=1.83e-01
Training epoch 100, Batch 1000/1000: LR=9.35e-05, Loss=2.59e-02 BER=9.99e-03 FER=1.82e-01
Epoch 100 Train Time 86.15270018577576s

Training epoch 101, Batch 500/1000: LR=9.34e-05, Loss=2.58e-02 BER=9.92e-03 FER=1.80e-01
Training epoch 101, Batch 1000/1000: LR=9.34e-05, Loss=2.58e-02 BER=9.94e-03 FER=1.80e-01
Epoch 101 Train Time 86.19270038604736s

Training epoch 102, Batch 500/1000: LR=9.32e-05, Loss=2.58e-02 BER=9.94e-03 FER=1.79e-01
Training epoch 102, Batch 1000/1000: LR=9.32e-05, Loss=2.59e-02 BER=9.96e-03 FER=1.80e-01
Epoch 102 Train Time 88.23212599754333s

Training epoch 103, Batch 500/1000: LR=9.31e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.82e-01
Training epoch 103, Batch 1000/1000: LR=9.31e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.81e-01
Epoch 103 Train Time 87.09704494476318s

Training epoch 104, Batch 500/1000: LR=9.30e-05, Loss=2.59e-02 BER=9.96e-03 FER=1.80e-01
Training epoch 104, Batch 1000/1000: LR=9.30e-05, Loss=2.59e-02 BER=1.00e-02 FER=1.80e-01
Epoch 104 Train Time 86.87220501899719s

Training epoch 105, Batch 500/1000: LR=9.28e-05, Loss=2.54e-02 BER=9.79e-03 FER=1.79e-01
Training epoch 105, Batch 1000/1000: LR=9.28e-05, Loss=2.56e-02 BER=9.87e-03 FER=1.79e-01
Epoch 105 Train Time 86.86886954307556s

Training epoch 106, Batch 500/1000: LR=9.27e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.81e-01
Training epoch 106, Batch 1000/1000: LR=9.27e-05, Loss=2.60e-02 BER=1.00e-02 FER=1.80e-01
Epoch 106 Train Time 86.80250525474548s

Training epoch 107, Batch 500/1000: LR=9.26e-05, Loss=2.62e-02 BER=1.01e-02 FER=1.82e-01
Training epoch 107, Batch 1000/1000: LR=9.26e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.81e-01
Epoch 107 Train Time 86.52029228210449s

Training epoch 108, Batch 500/1000: LR=9.24e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.81e-01
Training epoch 108, Batch 1000/1000: LR=9.24e-05, Loss=2.59e-02 BER=1.00e-02 FER=1.81e-01
Epoch 108 Train Time 86.50681209564209s

Training epoch 109, Batch 500/1000: LR=9.23e-05, Loss=2.59e-02 BER=9.99e-03 FER=1.80e-01
Training epoch 109, Batch 1000/1000: LR=9.23e-05, Loss=2.58e-02 BER=9.96e-03 FER=1.80e-01
Epoch 109 Train Time 86.57665872573853s

Training epoch 110, Batch 500/1000: LR=9.22e-05, Loss=2.58e-02 BER=9.97e-03 FER=1.83e-01
Training epoch 110, Batch 1000/1000: LR=9.22e-05, Loss=2.58e-02 BER=9.95e-03 FER=1.82e-01
Epoch 110 Train Time 86.5482771396637s

Training epoch 111, Batch 500/1000: LR=9.20e-05, Loss=2.58e-02 BER=9.94e-03 FER=1.79e-01
Training epoch 111, Batch 1000/1000: LR=9.20e-05, Loss=2.55e-02 BER=9.85e-03 FER=1.78e-01
Epoch 111 Train Time 86.52548265457153s

Training epoch 112, Batch 500/1000: LR=9.19e-05, Loss=2.63e-02 BER=1.02e-02 FER=1.81e-01
Training epoch 112, Batch 1000/1000: LR=9.19e-05, Loss=2.60e-02 BER=1.01e-02 FER=1.81e-01
Epoch 112 Train Time 86.63057446479797s

Training epoch 113, Batch 500/1000: LR=9.17e-05, Loss=2.61e-02 BER=1.00e-02 FER=1.82e-01
Training epoch 113, Batch 1000/1000: LR=9.17e-05, Loss=2.59e-02 BER=9.97e-03 FER=1.80e-01
Epoch 113 Train Time 86.33361148834229s

Training epoch 114, Batch 500/1000: LR=9.16e-05, Loss=2.61e-02 BER=1.01e-02 FER=1.82e-01
Training epoch 114, Batch 1000/1000: LR=9.16e-05, Loss=2.59e-02 BER=9.97e-03 FER=1.80e-01
Epoch 114 Train Time 86.23414969444275s

Training epoch 115, Batch 500/1000: LR=9.14e-05, Loss=2.56e-02 BER=9.89e-03 FER=1.79e-01
Training epoch 115, Batch 1000/1000: LR=9.14e-05, Loss=2.58e-02 BER=9.96e-03 FER=1.79e-01
Epoch 115 Train Time 86.74712371826172s

